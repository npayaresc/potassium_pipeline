# SHAP-Based Feature Selection Guide

## Overview

You can now train models using **only the top N most important features** identified by SHAP analysis. This creates a powerful iterative workflow:

1. Train a model with all features
2. Run SHAP analysis to identify top features
3. Retrain with only those top features for better performance and interpretability

## Why Use SHAP-Based Feature Selection?

### Benefits

1. **Improved Performance**: Removing irrelevant features can improve model accuracy
2. **Faster Training**: Fewer features = faster training and prediction
3. **Better Generalization**: Reduces overfitting by focusing on truly important features
4. **Interpretability**: Models with 30 features are much easier to understand than 495
5. **Reduced Noise**: Eliminates features that add noise without signal

### When to Use

- After initial model training and SHAP analysis
- When you have hundreds of features but suspect many are irrelevant
- To create simpler, more interpretable models
- To speed up training and inference
- To compare performance vs. full feature set

## Quick Start

### Complete Workflow

```bash
# Step 1: Train initial model with all features
uv run python main.py train --models lightgbm --strategy full_context --gpu --data-parallel --feature-parallel

# Step 2: Run SHAP analysis to identify top features
./run_shap_analysis.sh models/full_context_lightgbm_20251006_*.pkl

# Step 3: Train new model with only top 30 SHAP features
uv run python main.py train \
    --models xgboost lightgbm catboost \
    --strategy full_context \
    --shap-features models/full_context_lightgbm_*_shap_importance.csv \
    --shap-top-n 30 \
    --gpu --data-parallel --feature-parallel
```

### Results

After Step 3, you'll have models trained with only 30 features instead of 495:
- Faster training time
- Simpler models
- Often better performance
- Much easier to interpret

## Command-Line Arguments

### `--shap-features <path>`

**Required** to enable SHAP-based feature selection.

Path to SHAP importance CSV file generated by `run_shap_analysis.sh`.

```bash
--shap-features models/full_context_lightgbm_20251006_025844_shap_importance.csv
```

### `--shap-top-n <number>`

Number of top features to select. Default: 30

```bash
--shap-top-n 50    # Select top 50 features
--shap-top-n 20    # Select top 20 features
```

**Recommendations:**
- **30-50 features**: Good balance of performance and simplicity
- **20-30 features**: Highly interpretable, may sacrifice some accuracy
- **50-100 features**: More complex but potentially higher accuracy

### `--shap-min-importance <threshold>`

Optional minimum SHAP importance threshold. Only features with `shap_importance >= threshold` will be selected.

```bash
--shap-min-importance 0.001    # Only features with importance >= 0.001
```

**Note:** This is applied AFTER top-N selection, so you may get fewer than N features.

## Detailed Examples

### Example 1: Train with Top 30 Features

```bash
# First, ensure you have SHAP analysis results
./run_shap_analysis.sh models/full_context_lightgbm_20251006_025844.pkl

# Then train with top 30 features
uv run python main.py train \
    --models xgboost \
    --strategy full_context \
    --shap-features models/full_context_lightgbm_20251006_025844_shap_importance.csv \
    --shap-top-n 30 \
    --gpu
```

**Output:**
```
[SHAP FEATURE SELECTION] Enabled via command line:
  - SHAP file: models/full_context_lightgbm_20251006_025844_shap_importance.csv
  - Top N: 30

================================================================================
SHAP-BASED FEATURE SELECTION
================================================================================
SHAP importance file: full_context_lightgbm_20251006_025844_shap_importance.csv
Total features available: 495
Top N requested: 30
Features selected: 30

Top 10 selected features by SHAP importance:
   1. K_I_simple_peak_area                            SHAP=0.123456 (15.20%)
   2. K_I_simple_peak_height                          SHAP=0.098765 (12.10%)
   3. C_I_simple_peak_area                            SHAP=0.087654 (10.80%)
   ...

ðŸ“Š Selection Statistics:
  Selected features cover 78.5% of total SHAP importance
================================================================================
```

### Example 2: Compare Full vs. SHAP-Selected Features

```bash
# Train with all features (495)
uv run python main.py train \
    --models xgboost \
    --strategy full_context \
    --gpu

# Train with top 30 SHAP features
uv run python main.py train \
    --models xgboost \
    --strategy full_context \
    --shap-features models/full_context_lightgbm_*_shap_importance.csv \
    --shap-top-n 30 \
    --gpu

# Compare results in reports/training_summary_*.csv
```

### Example 3: Optimize with SHAP Features

```bash
# Hyperparameter optimization with SHAP-selected features
uv run python main.py optimize-models \
    --models xgboost lightgbm \
    --strategy full_context \
    --trials 200 \
    --shap-features models/full_context_lightgbm_*_shap_importance.csv \
    --shap-top-n 30 \
    --gpu
```

### Example 4: Multiple Strategies

```bash
# Train simple_only strategy with SHAP features from full_context
uv run python main.py train \
    --models ridge xgboost \
    --strategy simple_only \
    --shap-features models/full_context_lightgbm_*_shap_importance.csv \
    --shap-top-n 25
```

**Note:** SHAP file can be from a different strategy/model, as long as feature names overlap.

## How It Works

### 1. SHAP Analysis Creates Importance Rankings

When you run:
```bash
./run_shap_analysis.sh models/full_context_lightgbm_20251006_025844.pkl
```

It generates `models/full_context_lightgbm_20251006_025844_shap_importance.csv`:

```csv
feature,shap_importance,%_of_total,cumulative_%
K_I_simple_peak_area,0.123456,15.2,15.2
K_I_simple_peak_height,0.098765,12.1,27.3
C_I_simple_peak_area,0.087654,10.8,38.1
...
```

### 2. SHAPBasedFeatureSelector Filters Features

During training with `--shap-features`, the pipeline:
1. Extracts all features from the training data (e.g., 495 features)
2. Loads the SHAP importance file
3. Selects top N features based on SHAP rankings
4. Filters training data to keep only those features
5. Trains the model with reduced feature set

### 3. Models are Saved with Selected Features

The saved model pipeline includes the SHAP selector:
```
Pipeline:
  - shap_feature_selection (SHAPBasedFeatureSelector)
  - regressor (XGBRegressor)
```

When loading for prediction, it automatically selects the same features.

## Feature Selection Precedence

The pipeline supports multiple feature selection methods. Priority order:

1. **SHAP-based** (if `--shap-features` specified) - **HIGHEST PRIORITY**
2. Standard feature selection (if `use_feature_selection=True` in config)
3. No feature selection (all features)

Example:
```bash
# This will use SHAP selection (overrides config)
uv run python main.py train \
    --models xgboost \
    --shap-features models/..._shap_importance.csv \
    --shap-top-n 30

# Even if config has use_feature_selection=True, SHAP takes precedence
```

## Best Practices

### 1. Start with a Strong Base Model

Use a good model (XGBoost, LightGBM) for initial SHAP analysis:
```bash
uv run python main.py train --models lightgbm --strategy full_context --gpu
./run_shap_analysis.sh --latest lightgbm
```

### 2. Choose Appropriate N

Test different values:
```bash
# Try 20, 30, 50 features
for n in 20 30 50; do
    uv run python main.py train \
        --models xgboost \
        --shap-features models/..._shap_importance.csv \
        --shap-top-n $n \
        --gpu
done
```

Compare results to find sweet spot.

### 3. Verify Feature Coverage

Check the `ðŸ“Š Selection Statistics` in the output:
```
Selected features cover 78.5% of total SHAP importance
```

Aim for **70-85%** coverage with 30-50 features.

### 4. Compare Against Full Feature Set

Always train both:
- Full feature set (baseline)
- SHAP-selected features (comparison)

This validates that SHAP selection actually helps.

### 5. Cross-Validate Different SHAP Files

Try SHAP importance from different models:
```bash
# SHAP from LightGBM
./run_shap_analysis.sh --latest lightgbm

# SHAP from XGBoost
./run_shap_analysis.sh --latest xgboost

# Compare which gives better feature rankings
```

## Typical Results

### Full Context Strategy

- **Original features**: 495
- **Top 30 SHAP features**: ~75-80% of importance
- **Typical speedup**: 3-5x faster training
- **Performance**: Often similar or better RÂ²

### Simple Only Strategy

- **Original features**: 267
- **Top 30 SHAP features**: ~80-85% of importance
- **Typical speedup**: 2-3x faster training
- **Performance**: Usually maintains RÂ², sometimes improves

## Troubleshooting

### Error: "SHAP importance file not found"

```
FileNotFoundError: SHAP importance file not found: models/..._shap_importance.csv
Run SHAP analysis first: ./run_shap_analysis.sh <model_path>
```

**Solution:** Run SHAP analysis first:
```bash
./run_shap_analysis.sh models/your_model.pkl
```

### Error: "Feature name mismatch"

```
ValueError: Feature count mismatch: X has 495 features, but selector was fitted with 267 features
```

**Solution:** Use SHAP file from same strategy:
- `full_context` model â†’ use `full_context` SHAP file
- `simple_only` model â†’ use `simple_only` SHAP file

### Warning: "Only X features selected (requested N)"

```
âš ï¸  Only 25 features selected (requested 30).
This may occur if min_importance threshold is too high...
```

**Solution:**
- Remove `--shap-min-importance` flag
- Or lower the threshold
- Or request fewer features with `--shap-top-n`

### Performance Worse with SHAP Features

If SHAP-selected features perform worse:
1. Try different N values (20, 30, 40, 50)
2. Use SHAP from a different base model
3. Verify SHAP analysis was run correctly
4. Check if features align with domain knowledge

## Integration with Other Features

### With GPU Training

```bash
uv run python main.py train \
    --models xgboost lightgbm catboost \
    --shap-features models/..._shap_importance.csv \
    --shap-top-n 30 \
    --gpu    # GPU acceleration still works
```

### With Sample Exclusion

```bash
uv run python main.py train \
    --models xgboost \
    --shap-features models/..._shap_importance.csv \
    --shap-top-n 30 \
    --exclude-suspects reports/mislabel_analysis/suspicious_samples_min_confidence_2.csv
```

### With Parallel Processing

```bash
uv run python main.py train \
    --models xgboost \
    --shap-features models/..._shap_importance.csv \
    --shap-top-n 30 \
    --data-parallel --feature-parallel    # Parallel processing still works
```

All features are compatible and can be combined.

## Advanced Usage

### Iterative Refinement

```bash
# Round 1: Train with all features
uv run python main.py train --models lightgbm --strategy full_context --gpu

# Round 2: SHAP analysis
./run_shap_analysis.sh --latest lightgbm

# Round 3: Train with top 50 SHAP features
uv run python main.py train --models lightgbm --strategy full_context --shap-features models/..._shap_importance.csv --shap-top-n 50 --gpu

# Round 4: SHAP analysis on SHAP-selected model
./run_shap_analysis.sh --latest lightgbm

# Round 5: Train with top 30 from refined SHAP
uv run python main.py train --models lightgbm --strategy full_context --shap-features models/..._shap_importance.csv --shap-top-n 30 --gpu
```

Each iteration refines the feature set.

### Feature Importance Threshold

```bash
# Select top 50 features, but only if importance >= 0.001
uv run python main.py train \
    --models xgboost \
    --shap-features models/..._shap_importance.csv \
    --shap-top-n 50 \
    --shap-min-importance 0.001
```

This ensures features meet both criteria:
1. In top N
2. Above minimum importance threshold

## Summary

SHAP-based feature selection enables you to:
- âœ… Train models with only the most important features
- âœ… Improve performance and interpretability
- âœ… Reduce training time significantly
- âœ… Create simpler, more robust models
- âœ… Iterate and refine feature selection

**Typical workflow:**
1. Train baseline model with all features
2. Run SHAP analysis: `./run_shap_analysis.sh <model>`
3. Train with top features: `--shap-features ... --shap-top-n 30`
4. Compare and iterate

This creates a powerful iterative improvement cycle!
